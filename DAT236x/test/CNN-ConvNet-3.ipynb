{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 7910724 parameters in 8 parameter tensors.\n",
      "\n",
      "Learning rate per 1 samples: 0.001\n",
      "Momentum per 1 samples: 0.0\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.656208 * 15000, metric = 25.81% * 15000 13.715s (1093.7 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.165464 * 15000, metric = 5.67% * 15000 10.902s (1375.9 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.064348 * 15000, metric = 1.90% * 15000 10.908s (1375.1 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.032989 * 15000, metric = 0.92% * 15000 10.987s (1365.2 samples/s);\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.024236 * 15000, metric = 0.67% * 15000 11.003s (1363.3 samples/s);\n",
      "Momentum per 1 samples: 0.9990239141819757\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.015923 * 15000, metric = 0.37% * 15000 11.051s (1357.3 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.011784 * 15000, metric = 0.27% * 15000 11.089s (1352.7 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.007103 * 15000, metric = 0.14% * 15000 11.106s (1350.6 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.008172 * 15000, metric = 0.21% * 15000 11.110s (1350.1 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.006887 * 15000, metric = 0.09% * 15000 11.096s (1351.8 samples/s);\n",
      "\n",
      "Final Results: Minibatch[1-21]: errs = 0.22% * 5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import cntk as C\n",
    "\n",
    "# Paths relative to current python file.\n",
    "abs_path   = os.path.dirname(os.path.abspath(\"DAT264x\"))\n",
    "data_path  = os.path.join(abs_path, \"data\")\n",
    "model_path = os.path.join(abs_path, \"Models\")\n",
    "\n",
    "# Define the reader for both training and evaluation action.\n",
    "def create_reader(path, is_training, input_dim, label_dim):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        features=C.io.StreamDef(field='features', shape=input_dim),\n",
    "        labels=C.io.StreamDef(field='labels',   shape=label_dim)\n",
    "    )), randomize=is_training, max_sweeps=C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "# Creates and trains a feedforward classification model for MNIST images\n",
    "debug_output=False\n",
    "epoch_size=15000\n",
    "minibatch_size=64\n",
    "max_epochs=10\n",
    "image_height = 64\n",
    "image_width  = 64\n",
    "num_channels = 1\n",
    "input_dim = image_height * image_width * num_channels\n",
    "num_output_classes = 4\n",
    "\n",
    "# Input variables denoting the features and label data\n",
    "input_var = C.ops.input_variable((num_channels, image_height, image_width), np.float32)\n",
    "label_var = C.ops.input_variable(num_output_classes, np.float32)\n",
    "\n",
    "# Instantiate the feedforward classification model\n",
    "scaled_input = C.ops.element_times(C.ops.constant(0.00390625), input_var)\n",
    "\n",
    "with C.layers.default_options(activation=C.ops.relu, pad=False):\n",
    "    conv1 = C.layers.Convolution2D((3,3), 64, pad=True)(scaled_input)\n",
    "    conv2 = C.layers.Convolution2D((3,3), 64)(conv1)\n",
    "    pool1 = C.layers.MaxPooling((2,2), (2,2))(conv2)\n",
    "    drop1 = C.layers.Dropout(0.25)(pool1)\n",
    "    fc1   = C.layers.Dense(128)(drop1)\n",
    "    drop2 = C.layers.Dropout(0.5)(fc1)\n",
    "    z     = C.layers.Dense(num_output_classes, activation=None)(drop2)\n",
    "\n",
    "ce = C.losses.cross_entropy_with_softmax(z, label_var)\n",
    "pe = C.metrics.classification_error(z, label_var)\n",
    "\n",
    "reader_train = create_reader(os.path.join(data_path, 'Data-train-15000_20180720_070615.txt'), \n",
    "                             True, input_dim, num_output_classes)\n",
    "\n",
    "# Set learning parameters\n",
    "lr_per_sample    = [0.001]*10 + [0.0005]*10 + [0.0001]\n",
    "lr_schedule      = C.learning_parameter_schedule_per_sample(lr_per_sample, epoch_size=epoch_size)\n",
    "mms = [0]*5 + [0.9990239141819757]\n",
    "mm_schedule      = C.learners.momentum_schedule_per_sample(mms, epoch_size=epoch_size)\n",
    "\n",
    "# Instantiate the trainer object to drive the model training\n",
    "learner = C.learners.momentum_sgd(z.parameters, lr_schedule, mm_schedule)\n",
    "progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=max_epochs)\n",
    "trainer = C.Trainer(z, (ce, pe), learner, progress_printer)\n",
    "\n",
    "# define mapping from reader streams to network inputs\n",
    "input_map = {\n",
    "    input_var : reader_train.streams.features,\n",
    "    label_var : reader_train.streams.labels\n",
    "}\n",
    "\n",
    "C.logging.log_number_of_parameters(z) ; print()\n",
    "\n",
    "# Get minibatches of images to train with and perform model training\n",
    "for epoch in range(max_epochs):       # loop over epochs\n",
    "    sample_count = 0\n",
    "    while sample_count < epoch_size:  # loop over minibatches in the epoch\n",
    "        data = reader_train.next_minibatch(min(minibatch_size, epoch_size - sample_count), input_map=input_map) # fetch minibatch.\n",
    "        trainer.train_minibatch(data)                                   # update model with it\n",
    "        sample_count += data[label_var].num_samples                     # count samples processed so far\n",
    "\n",
    "    trainer.summarize_training_progress()\n",
    "    z.save(os.path.join(model_path, \"ConvNet_MNIST_{}.dnn\".format(epoch)))\n",
    "\n",
    "# Load test data\n",
    "reader_test = create_reader(os.path.join(data_path, 'Data-test-5000_20180720_070615.txt'), \n",
    "                            False, input_dim, num_output_classes)\n",
    "\n",
    "input_map = {\n",
    "    input_var : reader_test.streams.features,\n",
    "    label_var : reader_test.streams.labels\n",
    "}\n",
    "\n",
    "# Test data for trained model\n",
    "epoch_size = 5000\n",
    "minibatch_size = 250\n",
    "\n",
    "# process minibatches and evaluate the model\n",
    "metric_numer    = 0\n",
    "metric_denom    = 0\n",
    "sample_count    = 0\n",
    "minibatch_index = 0\n",
    "\n",
    "while sample_count < epoch_size:\n",
    "    current_minibatch = min(minibatch_size, epoch_size - sample_count)\n",
    "\n",
    "    # Fetch next test min batch.\n",
    "    data = reader_test.next_minibatch(current_minibatch, input_map=input_map)\n",
    "\n",
    "    # minibatch data to be trained with\n",
    "    metric_numer += trainer.test_minibatch(data) * current_minibatch\n",
    "    metric_denom += current_minibatch\n",
    "\n",
    "    # Keep track of the number of samples processed so far.\n",
    "    sample_count += data[label_var].num_samples\n",
    "    minibatch_index += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"Final Results: Minibatch[1-{}]: errs = {:0.2f}% * {}\".format(minibatch_index+1, (metric_numer*100.0)/metric_denom, metric_denom))\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric number:  11.0\n",
      "Metric denom:  5000\n",
      "Error rate:  0.0022\n"
     ]
    }
   ],
   "source": [
    "print('Metric number: ',metric_numer)\n",
    "print('Metric denom: ', metric_denom)\n",
    "print('Error rate: ', metric_numer/metric_denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = C.softmax(z)\n",
    "# Read the data for evaluation\n",
    "test_file=os.path.join(data_path, \"Test-10_20180720_064731.txt\")\n",
    "test_file2=os.path.join(data_path, \"Data-submit-20000_20180720_092205.txt\")\n",
    "reader_eval=create_reader(test_file2, False, input_dim, num_output_classes)\n",
    "\n",
    "x = C.input_variable(input_dim)\n",
    "y = C.input_variable(num_output_classes)\n",
    "eval_minibatch_size = 20000\n",
    "eval_input_map = {x: reader_eval.streams.features, y:reader_eval.streams.labels} \n",
    "\n",
    "data = reader_eval.next_minibatch(eval_minibatch_size, input_map=eval_input_map)\n",
    "\n",
    "img_label = data[y].asarray()\n",
    "img_data = data[x].asarray()\n",
    "\n",
    "# reshape img_data to: M x 1 x 64 x 64 to be compatible with model\n",
    "img_data = np.reshape(img_data, (eval_minibatch_size, 1, 64, 64))\n",
    "\n",
    "predicted_label_prob = [out.eval(img_data[i]) for i in range(len(img_data))]\n",
    "\n",
    "# Find the index with the maximum value for both predicted as well as the ground truth\n",
    "pred = [np.argmax(predicted_label_prob[i]) for i in range(len(predicted_label_prob))]\n",
    "gtlabel = [np.argmax(img_label[i]) for i in range(len(img_label))]\n",
    "#print(\"Label    :\", gtlabel[:25])\n",
    "#print(\"Predicted:\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  20000\n",
      "Count: 33\n",
      "Error: 0.00165\n",
      "Score: 0.99835\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] != gtlabel[i]:\n",
    "        count += 1\n",
    "print(\"Data: \", len(pred))\n",
    "print(\"Count:\", count)\n",
    "print(\"Error:\", count / len(pred))\n",
    "print(\"Score:\", (len(pred)- count) / len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "count = 0\n",
    "output = []\n",
    "with open('data/Test-keras-201807202100.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in reader:\n",
    "        if count == 0:\n",
    "            output.append('id,orientation\\n')\n",
    "        if count > 0:\n",
    "            id = row[0]\n",
    "            label = pred[count-1]\n",
    "            output.append('{},{}\\n'.format(id, label))\n",
    "            #print('({0}) file: {1} label: {2}'.format(count, row[0], row[1]) + ' onehot: ' + label)\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data/Submit-CNTK_20180720_094204.csv\n",
      "File save complete.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "filename = 'data/Submit-CNTK_' + datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S') + '.csv'\n",
    "print(\"Saving\", filename )\n",
    "with open(filename, 'w') as f:\n",
    "    for row in output:\n",
    "        f.write(row)\n",
    "print(\"File save complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
